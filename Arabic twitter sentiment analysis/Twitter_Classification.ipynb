{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(len(tf.config.list_physical_devices('GPU')))\n",
    "import torch\n",
    "print(torch.cuda.is_available())"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarabic.araby as ar\n",
    "\n",
    "# import Stemmer\n",
    "import re , emoji, functools, operator, string\n",
    "import torch , optuna, gc, random, os\n",
    "\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score, confusion_matrix, precision_score , recall_score\n",
    "from transformers import AutoConfig, AutoModelForSequenceClassification, AutoTokenizer\n",
    "from transformers.data.processors import SingleSentenceClassificationProcessor\n",
    "from transformers import Trainer , TrainingArguments\n",
    "from transformers.trainer_utils import EvaluationStrategy\n",
    "from transformers.data.processors.utils import InputFeatures\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.utils import resample\n",
    "\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.WARNING)\n",
    "logger = logging.getLogger(__name__)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load Dataset"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Y-BpP8TZugwK"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv(\"train.csv\")\n",
    "seed=0"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "b90o2vElugwT"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Arabic stop words"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "Zr_8LPo0ugwX"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "lJ0o2qs7ugwZ"
   },
   "outputs": [],
   "source": [
    "arabic_stop_words=[]\n",
    "with open ('list.txt',encoding='utf-8') as f :\n",
    "  for i in f.readlines() :\n",
    "    arabic_stop_words.append(i)\n",
    "    arabic_stop_words[-1]=arabic_stop_words[-1][:-1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "#============= Read CSV and apply data preperation =============#\n",
    "\n",
    "\n",
    "def data_preprocessing (data_frame):\n",
    "  # clean-up: remove #tags, http links and special symbols\n",
    "  data_frame['tweet']= data_frame['tweet'].apply(lambda x: x[2:-2])\n",
    "  data_frame['tweet']= data_frame['tweet'].apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'\"+', '', x))\n",
    "\n",
    "  # Remove arabic signs\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n",
    "\n",
    "  # Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n",
    "\n",
    "  # remove stop words\n",
    "  data_frame['tweet'] = data_frame['tweet'].apply(lambda x: '' if x in arabic_stop_words else x)\n",
    "\n",
    "  from nltk.stem.isri import ISRIStemmer\n",
    "  df['tweet']=df['tweet'].apply(lambda x:ISRIStemmer().stem(x))\n",
    "\n",
    "  return data_frame\n"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "O7VPzTXYugwo"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# st =  Stemmer.Stemmer('arabic')\n",
    "import string,emoji\n",
    "def data_cleaning (text):\n",
    "  text = re.sub(r'^https?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r'^http?:\\/\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n",
    "  text = re.sub(r\"http\\S+\", \"\", text)\n",
    "  text = re.sub(r\"https\\S+\", \"\", text)\n",
    "  text = re.sub(r'\\s+', ' ', text)\n",
    "  text = re.sub(\"(\\s\\d+)\",\"\",text)\n",
    "  text = re.sub(r\"$\\d+\\W+|\\b\\d+\\b|\\W+\\d+$\", \"\", text)\n",
    "  text = re.sub(\"\\d+\", \" \", text)\n",
    "  text = ar.strip_tashkeel(text)\n",
    "  text = ar.strip_tatweel(text)\n",
    "  text = text.replace(\"#\", \" \");\n",
    "  text = text.replace(\"@\", \" \");\n",
    "  text = text.replace(\"_\", \" \");\n",
    "  translator = str.maketrans('', '', string.punctuation)\n",
    "  text = text.translate(translator)\n",
    "  em = text\n",
    "  em_split_emoji = emoji.get_emoji_regexp().split(em)\n",
    "  em_split_whitespace = [substr.split() for substr in em_split_emoji]\n",
    "  em_split = functools.reduce(operator.concat, em_split_whitespace)\n",
    "  text = \" \".join(em_split)\n",
    "  text = re.sub(r'(.)\\1+', r'\\1', text)\n",
    "  # text_stem = \" \".join([st.stemWord(i) for i in text.split()])\n",
    "  # text = text +\" \"+ text_stem\n",
    "  text = text.replace(\"آ\", \"ا\")\n",
    "  text = text.replace(\"إ\", \"ا\")\n",
    "  text = text.replace(\"أ\", \"ا\")\n",
    "  text = text.replace(\"ؤ\", \"و\")\n",
    "  text = text.replace(\"ئ\", \"ي\")\n",
    "\n",
    "  return text"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "df=data_preprocessing(df)\n",
    "df['tweet']=df['tweet'].apply(lambda x: data_cleaning(x))\n",
    "df"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "v_gDzrkBwMz6",
    "outputId": "bfc9d862-242a-4733-86db-bc2449555531",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 7,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirolos Atef\\AppData\\Local\\Temp\\ipykernel_19372\\3188261332.py:20: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  em_split_emoji = emoji.get_emoji_regexp().split(em)\n"
     ]
    },
    {
     "data": {
      "text/plain": "                                                  tweet class\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...   pos\n1                                    كل سنة وانتم طيبين   pos\n2                                 و انتهى مشوار الخواجة   neg\n3                             مش عارف ابتدى مذاكره منين   neg\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...   neg\n...                                                 ...   ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...   neu\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...   neu\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...   pos\n2057                             انت متناقض جدا يا صلاح   neg\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب   neu\n\n[2059 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>neu</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>pos</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>neg</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>neu</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Label Encoder"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "tBEVTgBRugws"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet  class\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...      2\n1                                    كل سنة وانتم طيبين      2\n2                                 و انتهى مشوار الخواجة      0\n3                             مش عارف ابتدى مذاكره منين      0\n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0\n...                                                 ...    ...\n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1\n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1\n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2\n2057                             انت متناقض جدا يا صلاح      0\n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1\n\n[2059 rows x 2 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 2 columns</p>\n</div>"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "# Apply label encoding over the labels\n",
    "lable_encoder = preprocessing.LabelEncoder()\n",
    "encoded_labels =lable_encoder.fit_transform(df[\"class\"])\n",
    "df['class']=encoded_labels\n",
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "nvWAmR5nugwu",
    "outputId": "96f914bc-922a-413b-d765-f85b8f28dd57"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "data": {
      "text/plain": "34"
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['length']=df['tweet'].apply(lambda x:len(x.split(' ')))\n",
    "df['length'].max()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train Test Split"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "RscS77J9ugww"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "1826    هتفضل مشاكلي بيني و بين نفسي لحد ما اموت و هيف...\n1505             لو فشلنا ادينا حاولنا دا هيدي الامل لغير\n1994    اذا جاء اجل اله لايقدم ولا يوخر الموت اقرب الي...\n1349                      دايما بتاخد منا اعز واغلى ماعند\n781                      كان هيلعن اليوم الي كان فيه مصري\n                              ...                        \n596                                           امراه رايعه\n1887                        مارلين مونرو حلوة فشخ ليه كدا\n942                       تحت البطانيه و جنب شاحن الموبيل\n634                                            مساء العسل\n19            ها ايه رايك اقوله ايه بقا انا بقول مودي حلو\nName: tweet, Length: 412, dtype: object"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_validation, y_train, y_validation=train_test_split(df['tweet'], df['class'], test_size=0.2, random_state=seed)\n",
    "X_validation"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qhLxzIXvugwx",
    "outputId": "418c9aa9-789d-4708-ed5e-bb960fe2514b"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## TF_IDF"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "kevPnoxdugwy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def tfidf_ngram(n_gram,X_train,X_val):\n",
    "    vectorizer = TfidfVectorizer(ngram_range=(n_gram,n_gram))\n",
    "    x_train_vec = vectorizer.fit_transform(X_train)\n",
    "    x_test_vec = vectorizer.transform(X_val)\n",
    "    return x_train_vec,x_test_vec"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "roXYHxrdugwy"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "# Applying tfidf with 1-gram, 2-gram and 3-gram\n",
    "tfidf_1g_transformation_train,tfidf_1g_transformation_validation= tfidf_ngram(1,X_train,X_validation)\n",
    "tfidf_2g_transformation_train,tfidf_2g_transformation_validation= tfidf_ngram(2,X_train,X_validation)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "iuWEPExnugwz"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Machine learning models"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "7BYrD0RLugw0"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9878567091681846\n",
      "0.41262135922330095\n",
      "0.4596235579842137\n",
      "0.4077669902912621\n",
      "0.9878567091681846\n",
      "0.4223300970873786\n",
      "0.9878567091681846\n",
      "0.41262135922330095\n",
      "0.9878567091681846\n",
      "0.44902912621359226\n"
     ]
    }
   ],
   "source": [
    "from sklearn.svm import SVC\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "models=[SVC(),XGBClassifier(),RandomForestClassifier(),DecisionTreeClassifier(),LogisticRegression()]\n",
    "for m in models :\n",
    "    m.fit(tfidf_2g_transformation_train,y_train)\n",
    "    print(m.score(tfidf_2g_transformation_train,y_train))\n",
    "    print(m.score(tfidf_2g_transformation_validation,y_validation))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RCB9CYrwugw0",
    "outputId": "6f138f26-bddf-43cf-c2ca-8af7b36cefdd"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Model and Tokenizer initialization"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "546gZdstugw1"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (4.19.4)\n",
      "Requirement already satisfied: requests in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2.27.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (4.64.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (1.22.4)\n",
      "Requirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.12.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (5.4.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (0.7.0)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (21.3)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (2022.6.2)\n",
      "Requirement already satisfied: filelock in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from transformers) (3.7.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.2.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from packaging>=20.0->transformers) (3.0.9)\n",
      "Requirement already satisfied: colorama in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (3.3)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (1.26.9)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2022.5.18.1)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\users\\kirolos atef\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from requests->transformers) (2.0.12)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "1PPa1X-Tugw1",
    "outputId": "cbb39e64-a238-4dc1-c0e4-779c2fff682c"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at UBC-NLP/MARBERT were not used when initializing BertForSequenceClassification: ['cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at UBC-NLP/MARBERT and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel, AutoModelForSequenceClassification\n",
    "\n",
    "#============= Initialize Arabic Bert =============#\n",
    "#load your pre_trained model with all its weights\n",
    "# model_name= 'aubmindlab/bert-base-arabertv02'\n",
    "model_name='UBC-NLP/MARBERT' #top\n",
    "# model_name='asafaya/bert-base-arabic'\n",
    "tokenizer =AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=3)\n",
    "# model=AutoModel.from_pretrained(model_name,output_hidden_states=True)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WqHEVmquugw1",
    "outputId": "0200298a-c7e6-419e-aea0-f6260a11b95e"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# Tokenize the sentences using bert tokenizer\n",
    "df[\"bert_tokens\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())\n",
    "df[\"bert_tokens_ids\"] = df.tweet.apply(lambda x: tokenizer(x).tokens())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "sc5VRJOCugw2"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet  class  length  \\\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...      2      25   \n1                                    كل سنة وانتم طيبين      2       4   \n2                                 و انتهى مشوار الخواجة      0       4   \n3                             مش عارف ابتدى مذاكره منين      0       5   \n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0      20   \n...                                                 ...    ...     ...   \n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1      10   \n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1      11   \n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2      16   \n2057                             انت متناقض جدا يا صلاح      0       5   \n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1       8   \n\n                                            bert_tokens  \\\n0     [[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...   \n1                 [[CLS], كل, سنة, وانتم, طيبين, [SEP]]   \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]   \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...   \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]   \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...   \n\n                                        bert_tokens_ids  \n0     [[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...  \n1                 [[CLS], كل, سنة, وانتم, طيبين, [SEP]]  \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]  \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]  \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...  \n...                                                 ...  \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...  \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...  \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...  \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]  \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...  \n\n[2059 rows x 5 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>length</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>2</td>\n      <td>25</td>\n      <td>[[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...</td>\n      <td>[[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>2</td>\n      <td>4</td>\n      <td>[[CLS], كل, سنة, وانتم, طيبين, [SEP]]</td>\n      <td>[[CLS], كل, سنة, وانتم, طيبين, [SEP]]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n      <td>4</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n      <td>20</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n      <td>10</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n      <td>11</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n      <td>16</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n      <td>8</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 5 columns</p>\n</div>"
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 502
    },
    "id": "RkTkQeWbugw2",
    "outputId": "73f38be7-080e-49d8-ff9c-2d528830377d"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "df[\"encoded\"] = df.tweet.apply(lambda x: tokenizer.encode_plus(x,return_tensors='pt')['input_ids'])"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "0xw8-QmIugw3"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                  tweet  class  length  \\\n0     ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...      2      25   \n1                                    كل سنة وانتم طيبين      2       4   \n2                                 و انتهى مشوار الخواجة      0       4   \n3                             مش عارف ابتدى مذاكره منين      0       5   \n4     اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...      0      20   \n...                                                 ...    ...     ...   \n2054  الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...      1      10   \n2055  نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...      1      11   \n2056  ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...      2      16   \n2057                             انت متناقض جدا يا صلاح      0       5   \n2058     منطقة السيدة زينب ليلة المولد مسجد السيدة زينب      1       8   \n\n                                            bert_tokens  \\\n0     [[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...   \n1                 [[CLS], كل, سنة, وانتم, طيبين, [SEP]]   \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]   \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...   \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]   \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...   \n\n                                        bert_tokens_ids  \\\n0     [[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...   \n1                 [[CLS], كل, سنة, وانتم, طيبين, [SEP]]   \n2              [[CLS], و, انتهى, مشوار, الخواجة, [SEP]]   \n3         [[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]   \n4     [[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...   \n...                                                 ...   \n2054  [[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...   \n2055  [[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...   \n2056  [[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...   \n2057         [[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]   \n2058  [[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...   \n\n                                                encoded  \n0     [[tensor(2), tensor(1946), tensor(2468), tenso...  \n1     [[tensor(2), tensor(2009), tensor(3171), tenso...  \n2     [[tensor(2), tensor(144), tensor(7609), tensor...  \n3     [[tensor(2), tensor(2093), tensor(3323), tenso...  \n4     [[tensor(2), tensor(22181), tensor(1958), tens...  \n...                                                 ...  \n2054  [[tensor(2), tensor(4770), tensor(68899), tens...  \n2055  [[tensor(2), tensor(39939), tensor(3715), tens...  \n2056  [[tensor(2), tensor(3735), tensor(4880), tenso...  \n2057  [[tensor(2), tensor(2030), tensor(27008), tens...  \n2058  [[tensor(2), tensor(5627), tensor(16281), tens...  \n\n[2059 rows x 6 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>tweet</th>\n      <th>class</th>\n      <th>length</th>\n      <th>bert_tokens</th>\n      <th>bert_tokens_ids</th>\n      <th>encoded</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ان الذين يعيشون على الارض ليسوا ملايكة بل بشر ...</td>\n      <td>2</td>\n      <td>25</td>\n      <td>[[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...</td>\n      <td>[[CLS], ان, الذين, يعيشون, على, الارض, ليسوا, ...</td>\n      <td>[[tensor(2), tensor(1946), tensor(2468), tenso...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>كل سنة وانتم طيبين</td>\n      <td>2</td>\n      <td>4</td>\n      <td>[[CLS], كل, سنة, وانتم, طيبين, [SEP]]</td>\n      <td>[[CLS], كل, سنة, وانتم, طيبين, [SEP]]</td>\n      <td>[[tensor(2), tensor(2009), tensor(3171), tenso...</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>و انتهى مشوار الخواجة</td>\n      <td>0</td>\n      <td>4</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n      <td>[[CLS], و, انتهى, مشوار, الخواجة, [SEP]]</td>\n      <td>[[tensor(2), tensor(144), tensor(7609), tensor...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>مش عارف ابتدى مذاكره منين</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n      <td>[[CLS], مش, عارف, ابتدى, مذاكره, منين, [SEP]]</td>\n      <td>[[tensor(2), tensor(2093), tensor(3323), tenso...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>اختصروا الطريق بدلا من اختيار المنصف ثم الانقل...</td>\n      <td>0</td>\n      <td>20</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[CLS], اختصر, ##وا, الطريق, بدلا, من, اختيار,...</td>\n      <td>[[tensor(2), tensor(22181), tensor(1958), tens...</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>2054</th>\n      <td>الجمال مبيحتاح اي مكياج لناعم وله خشن جمل الطا...</td>\n      <td>1</td>\n      <td>10</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[CLS], الجمال, مبيح, ##تاح, اي, مكياج, لنا, #...</td>\n      <td>[[tensor(2), tensor(4770), tensor(68899), tens...</td>\n    </tr>\n    <tr>\n      <th>2055</th>\n      <td>نتمني وجود الفنانة رنا سماحة افضل فنانة صاعدة ...</td>\n      <td>1</td>\n      <td>11</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[CLS], نتمني, وجود, الفنانة, رنا, سماحة, افضل...</td>\n      <td>[[tensor(2), tensor(39939), tensor(3715), tens...</td>\n    </tr>\n    <tr>\n      <th>2056</th>\n      <td>ولد الهدى فالكاينات ضياء وفم الزمان تبسم وسناء...</td>\n      <td>2</td>\n      <td>16</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n      <td>[[CLS], ولد, الهدى, فالك, ##اينات, ضياء, وف, #...</td>\n      <td>[[tensor(2), tensor(3735), tensor(4880), tenso...</td>\n    </tr>\n    <tr>\n      <th>2057</th>\n      <td>انت متناقض جدا يا صلاح</td>\n      <td>0</td>\n      <td>5</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n      <td>[[CLS], انت, متناقض, جدا, يا, صلاح, [SEP]]</td>\n      <td>[[tensor(2), tensor(2030), tensor(27008), tens...</td>\n    </tr>\n    <tr>\n      <th>2058</th>\n      <td>منطقة السيدة زينب ليلة المولد مسجد السيدة زينب</td>\n      <td>1</td>\n      <td>8</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n      <td>[[CLS], منطقة, السيدة, زينب, ليلة, المولد, مسج...</td>\n      <td>[[tensor(2), tensor(5627), tensor(16281), tens...</td>\n    </tr>\n  </tbody>\n</table>\n<p>2059 rows × 6 columns</p>\n</div>"
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 641
    },
    "id": "34QQvxBhugw3",
    "outputId": "7a4279c0-81a7-4fd3-e22a-158fffd8d130"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Padding and attention mask"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "fhb0-jQMugw4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from keras_preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway.\n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 64\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids = [tokenizer.convert_tokens_to_ids(x) for x in df['bert_tokens']]\n",
    "# Pad our input tokens\n",
    "input_ids = pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "taCZLE63ugw4"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Use train_test_split to split our data into train and validation sets for training\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, encoded_labels,\n",
    "                                                            random_state=seed, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=seed, test_size=0.1)\n",
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "# Select a batch size for training. For fine-tuning BERT on a specific task, the authors recommend a batch size of 16 or 32\n",
    "batch_size = 80\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop,\n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "\n",
    "train_data = TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size)\n",
    "\n",
    "validation_data = TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_dataloader = DataLoader(validation_data, batch_size=batch_size)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "ip_wd-t9ugw5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Set optimizer parameters"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "CEvllsdYugw5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [{'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],'weight_decay_rate': 0.01},\n",
    "                                {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],'weight_decay_rate': 0.0}]\n",
    "# This variable contains all of the hyperparemeter information our training loop needs\n",
    "# optimizer = optim.BertAdam(optimizer_grouped_parameters,lr=2e-5,warmup=.1)\n",
    "optimizer = optim.AdamW(optimizer_grouped_parameters,lr=5e-6)"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "SIutfqrnugw5"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Training"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    },
    "id": "sZZCYaUKugw5"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/20 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.007775241509079933\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   5%|▌         | 1/20 [00:10<03:25, 10.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7193840579710145\n",
      "Train loss: 0.010197977254089588\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  10%|█         | 2/20 [00:21<03:13, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7152173913043477\n",
      "Train loss: 0.006798163851878296\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  15%|█▌        | 3/20 [00:32<03:02, 10.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7121376811594202\n",
      "Train loss: 0.005740032240282744\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  20%|██        | 4/20 [00:43<02:52, 10.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7266304347826087\n",
      "Train loss: 0.006551246313999097\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  25%|██▌       | 5/20 [00:53<02:42, 10.80s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7121376811594202\n",
      "Train loss: 0.008503883931552991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  30%|███       | 6/20 [01:04<02:29, 10.69s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7121376811594202\n",
      "Train loss: 0.004823497952505325\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  35%|███▌      | 7/20 [01:14<02:18, 10.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7266304347826087\n",
      "Train loss: 0.008328444562115086\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  40%|████      | 8/20 [01:25<02:06, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.716304347826087\n",
      "Train loss: 0.00453309793859565\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  45%|████▌     | 9/20 [01:35<01:56, 10.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7235507246376812\n",
      "Train loss: 0.0037235923179347688\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  50%|█████     | 10/20 [01:46<01:45, 10.53s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7235507246376812\n",
      "Train loss: 0.0027813007618533447\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  55%|█████▌    | 11/20 [01:56<01:34, 10.51s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7307971014492755\n",
      "Train loss: 0.00383834765428522\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  60%|██████    | 12/20 [02:07<01:24, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6954710144927536\n",
      "Train loss: 0.008061746911456188\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  65%|██████▌   | 13/20 [02:17<01:13, 10.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7327898550724639\n",
      "Train loss: 0.008963529883961504\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  70%|███████   | 14/20 [02:28<01:02, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7121376811594202\n",
      "Train loss: 0.006504351966820347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  75%|███████▌  | 15/20 [02:38<00:52, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7016304347826087\n",
      "Train loss: 0.0068676988982285065\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  80%|████████  | 16/20 [02:49<00:41, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6954710144927536\n",
      "Train loss: 0.002551882231879669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  85%|████████▌ | 17/20 [02:59<00:31, 10.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6913043478260869\n",
      "Train loss: 0.006357304368672582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  90%|█████████ | 18/20 [03:10<00:20, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7027173913043478\n",
      "Train loss: 0.003932676801923662\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:  95%|█████████▌| 19/20 [03:20<00:10, 10.48s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.6996376811594204\n",
      "Train loss: 0.0027347728464519605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 100%|██████████| 20/20 [03:31<00:00, 10.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.7079710144927537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm, trange\n",
    "# Function to calculate the accuracy of our predictions vs labels\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
    "t = []\n",
    "\n",
    "# Store our loss and accuracy for plotting\n",
    "train_loss_set = []\n",
    "\n",
    "# Number of training epochs\n",
    "epochs = 20\n",
    "\n",
    "# Transfer the model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "# trange is a tqdm wrapper around the normal python range\n",
    "for _ in trange(epochs, desc=\"Epoch\"):\n",
    "\n",
    "\n",
    "  # Training\n",
    "\n",
    "  # Set our model to training mode (as opposed to evaluation mode)\n",
    "  model.train()\n",
    "\n",
    "  # Tracking variables\n",
    "  tr_loss = 0\n",
    "  nb_tr_examples, nb_tr_steps = 0, 0\n",
    "\n",
    "  # Train the data for one epoch\n",
    "  for step, batch in enumerate(train_dataloader):\n",
    "    # Add batch to GPU\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n",
    "    # Clear out the gradients (by default they accumulate)\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # Forward pass\n",
    "    loss = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"), labels=b_labels.to(\"cuda\"))[\"loss\"]\n",
    "    train_loss_set.append(loss.item())\n",
    "\n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "\n",
    "    # Update parameters and take a step using the computed gradient\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    # Update tracking variables\n",
    "    tr_loss += loss.item()\n",
    "    nb_tr_examples += b_input_ids.size(0)\n",
    "    nb_tr_steps += 1\n",
    "\n",
    "  print(\"Train loss: {}\".format(tr_loss/nb_tr_steps))\n",
    "\n",
    "  # Validation\n",
    "\n",
    "  # Put model in evaluation mode to evaluate loss on the validation set\n",
    "  model.eval()\n",
    "\n",
    "  # Tracking variables\n",
    "  eval_loss, eval_accuracy = 0, 0\n",
    "  nb_eval_steps, nb_eval_examples = 0, 0\n",
    "\n",
    "  # Evaluate data for one epoch\n",
    "  for batch in validation_dataloader:\n",
    "    # Add batch to GPU\n",
    "    # batch = tuple(t.to(device) for t in batch)\n",
    "    # Unpack the inputs from our dataloader\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_labels = b_labels.type(torch.LongTensor)   # casting to long\n",
    "    # Telling the model not to compute or store gradients, saving memory and speeding up validation\n",
    "    with torch.no_grad():\n",
    "      # Forward pass, calculate logit predictions\n",
    "      logits = model(b_input_ids.to(\"cuda\"), token_type_ids=None, attention_mask=b_input_mask.to(\"cuda\"))\n",
    "\n",
    "    # Move logits and labels to CPU\n",
    "    logits = logits[\"logits\"].detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "\n",
    "    tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "\n",
    "    eval_accuracy += tmp_eval_accuracy\n",
    "    nb_eval_steps += 1\n",
    "\n",
    "  print(\"Validation Accuracy: {}\".format(eval_accuracy/nb_eval_steps))"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0kfOowbxugw6",
    "outputId": "55691992-c8a4-4b2d-9e61-1074177eb38b"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Kirolos Atef\\AppData\\Local\\Temp\\ipykernel_19372\\3188261332.py:20: DeprecationWarning: 'emoji.get_emoji_regexp()' is deprecated and will be removed in version 2.0.0. If you want to remove emoji from a string, consider the method emoji.replace_emoji(str, replace='').\n",
      "To hide this warning, pin/downgrade the package to 'emoji~=1.6.3'\n",
      "  em_split_emoji = emoji.get_emoji_regexp().split(em)\n"
     ]
    }
   ],
   "source": [
    "#============= Read CSV and apply data preperation =============#\n",
    "df_submit = pd.read_csv(\"test.csv\")\n",
    "\n",
    "# # clean-up: remove #tags, http links and special symbols\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: x[2:-2])\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'http\\S+', '', x))\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'[@|#]\\S*', '', x))\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'\"+', '', x))\n",
    "#\n",
    "# # Remove arabic signs\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: re.sub(r'([@A-Za-z0-9_ـــــــــــــ]+)|[^\\w\\s]|#|http\\S+', '', x))\n",
    "#\n",
    "# # Remove repeated letters like \"الللللللللللللللله\" to \"الله\"\n",
    "# df_submit.tweet = df_submit.tweet.apply(lambda x: x[0:2] + ''.join([x[i] for i in range(2, len(x)) if x[i]!=x[i-1] or x[i]!=x[i-2]]))\n",
    "#\n",
    "# # remove stop words\n",
    "# df_submit.iloc[:,0] = df_submit.iloc[:,0].apply(lambda x: '' if x in arabic_stop_words else x)\n",
    "df_submit=data_preprocessing(df_submit)\n",
    "\n",
    "df_submit[\"tweet\"] = df_submit.tweet.apply(lambda x: data_cleaning(x))\n",
    "\n",
    "# Tokenize the sentences using bert tokenizer\n",
    "df_submit[\"bert_tokens\"] = df_submit.tweet.apply(lambda x: tokenizer(x).tokens())"
   ],
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "id": "yJ2XSmwSugw6"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "bert_tokens_submit = df_submit[\"bert_tokens\"]"
   ],
   "metadata": {
    "id": "G7iPpop4ytzT",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 39,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Set the maximum sequence length. The longest sequence in our training set is 47, but we'll leave room on the end anyway. \n",
    "# In the original paper, the authors used a length of 512.\n",
    "MAX_LEN = 64\n",
    "# Use the BERT tokenizer to convert the tokens to their index numbers in the BERT vocabulary\n",
    "input_ids_submit = [tokenizer.convert_tokens_to_ids(x) for x in bert_tokens_submit]\n",
    "# Pad our input tokens\n",
    "input_ids_submit = pad_sequences(input_ids_submit, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "# Create attention masks\n",
    "attention_masks_submit = []\n",
    "\n",
    "# Create a mask of 1s for each token followed by 0s for padding\n",
    "for seq in input_ids_submit:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks_submit.append(seq_mask)"
   ],
   "metadata": {
    "id": "Z1vJgu5f0A-3",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 40,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Convert all of our data into torch tensors, the required datatype for our model\n",
    "inputs_submit = torch.tensor(input_ids_submit)\n",
    "masks_submit = torch.tensor(attention_masks_submit)\n",
    "\n",
    "# Create an iterator of our data with torch DataLoader. This helps save on memory during training because, unlike a for loop, \n",
    "# with an iterator the entire dataset does not need to be loaded into memory\n",
    "batch_size = 64\n",
    "submit_data = TensorDataset(inputs_submit, masks_submit)\n",
    "\n",
    "# do not use shuffle, we need the preds to be in same order\n",
    "submit_dataloader = DataLoader(submit_data, batch_size=batch_size)#, shuffle=True)"
   ],
   "metadata": {
    "id": "HUeIG5WD0M9n",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 41,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Put the model in an evaluation state\n",
    "model.eval()\n",
    "\n",
    "# Transfer model to GPU\n",
    "model.to(\"cuda\")\n",
    "\n",
    "outputs = []\n",
    "for input, masks in submit_dataloader:\n",
    "  torch.cuda.empty_cache() # empty the gpu memory\n",
    "\n",
    "  # Transfer the batch to gpu\n",
    "  input = input.to('cuda')\n",
    "  masks = masks.to('cuda')\n",
    "\n",
    "  # Run inference on the batch\n",
    "  output = model(input, attention_mask=masks)[\"logits\"]\n",
    "\n",
    "  # Transfer the output to CPU again and convert to numpy\n",
    "  output = output.cpu().detach().numpy()\n",
    "\n",
    "  # Store the output in a list\n",
    "  outputs.append(output)\n",
    "\n",
    "# Concatenate all the lists within the list into one list\n",
    "outputs = [x for y in outputs for x in y]\n",
    "\n",
    "# Inverse transform the label encoding\n",
    "pred_flat = np.argmax(outputs, axis=1).flatten()\n",
    "output_labels = lable_encoder.inverse_transform(pred_flat)"
   ],
   "metadata": {
    "id": "K-32QySM0Qpu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "submission = pd.DataFrame({\"Id\":np.arange(1, len(output_labels)+1), \"class\":output_labels})\n",
    "# save (submission)\n",
    "submission.to_csv(\"submission18.csv\", index=False)"
   ],
   "metadata": {
    "id": "Az3ccyKv0SYu",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "NAo2Q_2F06Je",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 29,
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  },
  "colab": {
   "name": "Twitter_Classification.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "accelerator": "GPU",
  "gpuClass": "standard"
 },
 "nbformat": 4,
 "nbformat_minor": 0
}